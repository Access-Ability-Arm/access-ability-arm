# Depth Cameras in Robotic Pick and Place: Implementation Guide for RealSense D435 and UFactory Lite 6

Vision-guided robotic manipulation has matured significantly in 2025, with depth cameras enabling sub-centimeter accuracy in pick and place tasks. For your assistive robotics application using the Intel RealSense D435 and UFactory Lite 6 arm, you have two viable implementation paths: a complete ROS 2 stack with MoveIt 2 that provides industrial-grade capabilities but requires 2-4 weeks to learn, or a streamlined Python approach using the official UFACTORY vision repository that can be operational in days. The D435 excels at the 0.3-1.0m working distance typical for tabletop manipulation, achieving 2-5mm accuracy with proper calibration, while the Lite 6's excellent SDK support and official depth camera integration examples significantly reduce development time compared to platforms without such resources. Modern learning-based grasp detection methods like GR-ConvNet achieve 98%+ accuracy at 20ms inference time, while the complete perception-to-execution pipeline typically runs at 10-15 Hz—sufficient for assistive robotics applications where reliability matters more than cycle time.

## RealSense D435 integration provides the foundation

The Intel RealSense D435 depth camera delivers precisely what tabletop manipulation requires. Its 50mm stereo baseline enables depth sensing from 0.3m to 3m, with accuracy better than 5mm at the 0.5-0.8m working distance ideal for the Lite 6's 440mm reach. The global shutter eliminates motion blur during robot movement, while the 87°×58° field of view captures sufficient workspace without requiring wide camera placement.

**Technical specifications matter for your planning.** The D435 streams 848×480 depth at 30 FPS with total system latency around 130ms—adequate for manipulation where planning time dominates. The camera draws 700mA at 5V via USB 3.1, adding negligible power requirements. Its compact 90×25×25mm form factor and 72g weight suit eye-in-hand mounting on the Lite 6 without significantly affecting payload capacity.

**The pyrealsense2 SDK provides straightforward integration.** After pip installation, a basic capture loop requires just 10 lines of code. The critical component for manipulation is depth-to-color alignment, which ensures every depth pixel corresponds to the same point in the color image—essential for combining RGB-based object detection with depth information for 3D localization. The SDK's built-in filters improve data quality: spatial filtering for edge-preserving smoothing, temporal filtering for noise reduction across frames, and hole-filling for invalid pixels. For pick and place, a conservative filter configuration balances quality and latency.

The camera's limitations shape system design. Transparent and highly reflective objects remain challenging for active stereo systems—the IR pattern either passes through or creates spurious reflections. Very dark surfaces reflect IR poorly, reducing signal quality. Direct sunlight overwhelms the IR projector pattern, though the D435f variant with IR filter mitigates this for outdoor applications. Beyond 3m, depth error increases quadratically, making the D435 unsuitable for long-range manipulation. The 0.2-0.3m minimum range also restricts very close object inspection.

**The optimal configuration for your setup pairs 848×480 resolution at 30 FPS with the High Density visual preset.** This balances point cloud quality with processing speed, providing approximately 400,000 points per frame before downsampling. For eye-in-hand mounting on the Lite 6, position the camera 50-100mm offset from the gripper center with a 15-30° downward tilt. This prevents gripper occlusion while maintaining good view angles for grasp planning.

## Point cloud processing transforms depth data into grasp candidates

Raw depth images contain 300,000+ noisy points spanning the entire camera view. The processing pipeline systematically filters, segments, and extracts graspable objects, reducing this to perhaps 10-20 candidate grasp poses in 100-200ms.

**The pipeline follows five stages with clear purpose.** First, point cloud generation converts aligned depth and RGB frames into 3D coordinates using the camera's intrinsic parameters—focal lengths fx, fy and principal point cx, cy. The RealSense SDK handles this efficiently via rs2.pointcloud.calculate(). Second, preprocessing removes irrelevant data: passthrough filtering crops to a workspace bounding box (removing 50-70% of points immediately), voxel grid downsampling reduces density to manageable levels (typically 0.01m voxel size), and outlier removal cleans noise. Third, plane segmentation uses RANSAC to detect and remove the dominant plane—usually the table surface. Fourth, Euclidean clustering groups the remaining points into individual objects. Finally, object detection extracts centroids, oriented bounding boxes, and surface normals for each cluster.

**The most critical parameter is clustering tolerance.** Setting this to 0.02m (2cm) works well for typical tabletop objects—smaller values split single objects into fragments, larger values merge separate items. Open3D's DBSCAN clustering with eps=0.02 and min_points=10 provides the right balance. Similarly, voxel downsampling at 0.01m preserves object features while reducing computational load by 90-95%.

Open3D has emerged as the preferred library for Python-based systems. Its modern API, excellent documentation, and fast C++ backend make it more accessible than PCL while providing sufficient functionality for manipulation. The complete preprocessing pipeline requires just 20 lines of Python and processes a 640×480 point cloud in 50-100ms on standard hardware. PCL remains superior for production C++ systems and ROS integration, offering more algorithms and tighter integration with the ROS ecosystem, but demands greater expertise.

**The optimized processing pipeline achieves real-time performance.** Early ROI filtering provides 2-3× speedup by immediately discarding the workspace exterior. Conservative downsampling (0.015m rather than 0.005m) trades minimal feature loss for substantial speed gains. Radius outlier removal runs faster than statistical outlier removal while providing adequate noise rejection for manipulation. This configuration processes frames at 10-15 Hz—sufficient for pick and place where object detection happens between robot movements, not during motion.

Common issues have known solutions. Noise from depth discontinuities at object edges responds to spatial filtering with moderate strength settings. Missing data in reflective or dark regions often doesn't prevent grasping if enough surface remains visible. For challenging materials, multiple viewpoints or multi-modal approaches combining depth with RGB features improve robustness. The key insight: manipulation rarely requires perfect reconstruction—partial point clouds often suffice if they capture graspable regions.

## Learning-based grasp detection surpasses geometric methods

The evolution from analytical approaches to neural networks has transformed grasp detection. While geometric methods like force-closure analysis remain valuable for understanding grasp quality, learning-based approaches now achieve superior performance on novel objects.

**GR-ConvNet represents the current state-of-the-art for planar grasping.** This fully convolutional network processes depth images directly, outputting pixel-wise grasp quality, angle, and width maps in a single forward pass. The key innovation: generating grasps densely across the image rather than sampling discrete candidates, enabling real-time closed-loop control. GR-ConvNet achieves 98.8% accuracy on the Cornell dataset and 95.1% on Jacquard, with real robot success rates exceeding 95% on household objects and 93% on adversarial items. Inference takes approximately 20ms on GPU, making it viable for responsive systems.

The architecture builds on residual connections and generative training, avoiding the sampling bottleneck of classification-based approaches. Pretrained models trained on Cornell or Jacquard datasets generalize reasonably well to novel objects—typically 80-85% success without fine-tuning. For your application, this means you can deploy a working system quickly using existing models, then fine-tune on your specific objects if needed.

**For 6-DOF grasping in cluttered scenes, the GraspNet-1Billion ecosystem provides comprehensive solutions.** This massive dataset contains 97,280 RGB-D images with over one billion grasp annotations, trained with real cluttered scenes rather than synthetic data. The baseline methods achieve strong performance, while recent approaches like Contact-GraspNet offer efficient contact-based prediction optimized for dense clutter. These methods directly process point clouds, handling occlusion and partial views naturally since they operate on 3D geometry rather than 2D projections.

The UFACTORY vision repository includes working GGCNN (predecessor to GR-ConvNet) implementations specifically for the Lite 6 with RealSense cameras. This provides an immediate starting point: the repository contains complete Python scripts that integrate camera capture, grasp detection, and robot control. You can be running actual pick and place within hours of setup, then iterate on improvements.

**Understanding grasp quality metrics helps debug failures.** The Ferrari-Canny epsilon metric quantifies the minimum disturbance wrench that could break a grasp—larger values indicate more robust grasps. Force closure tests whether contact forces can resist arbitrary external wrenches. In practice, learning-based methods implicitly learn these concepts from training data rather than computing them explicitly. The network's grasp quality output serves as a learned heuristic that correlates with physical stability.

Datasets enable the learning revolution. Cornell's 885 images provided the initial benchmark but limited diversity. Jacquard scaled to 54,000 synthetic images, dramatically improving generalization. GraspNet-1Billion brought real-world scale and clutter. For your assistive robotics application, starting with models pretrained on these datasets provides strong baselines. Domain-specific fine-tuning on 100-500 examples of your target objects can boost success rates by 10-20%.

## Grasp point selection balances multiple objectives

Detecting potential grasps is only half the challenge—selecting which grasp to execute requires reasoning about quality, reachability, and task requirements.

**Force closure remains the fundamental quality criterion.** A grasp achieves force closure when it can resist any external wrench through contact forces alone, without requiring specific object properties. For parallel-jaw grippers, this typically requires antipodal contact points—positions on opposite sides of the object with surface normals pointing toward each other. The mathematical condition involves testing whether the grasp wrench space contains the origin, computed via the grasp matrix that maps contact forces to object wrenches.

In practice, learning-based methods handle this implicitly. GQ-CNN (Grasp Quality CNN) learns to predict grasp success probability from 50 million synthetic examples, achieving 93% success on known objects and 80-85% on novel objects. The network outputs a score between 0 and 1 for each candidate grasp, enabling straightforward ranking. FC-GQ-CNN extends this to evaluate entire depth images densely, processing millions of candidates in parallel in approximately 625ms.

**Reachability filtering prevents wasted computation.** The highest-quality grasp is worthless if the robot cannot reach it. Practical systems implement hierarchical filtering: geometric pre-filtering based on workspace bounds reduces candidates by 90%, then inverse kinematics checks on the top 20-50 candidates identify actually reachable configurations. Collision checking ensures both the grasp configuration and approach trajectory remain obstacle-free. This staged approach balances thoroughness with computational efficiency.

The integration with motion planning significantly impacts success rates. Sequential approaches—plan grasp, then plan motion—often fail when high-quality grasps prove unreachable. Integrated methods like Grasp-RRT simultaneously grow a motion tree while evaluating grasp hypotheses, naturally ensuring selected grasps have collision-free access paths. For production systems, the extra planning complexity typically proves worthwhile through higher success rates and fewer recovery scenarios.

**Multi-metric combinations improve robustness beyond single-objective optimization.** Rather than selecting the highest-quality grasp, practical systems combine normalized scores for force closure, reachability, task alignment, and collision margin. Weighted combinations like 0.4×quality + 0.3×reachability + 0.2×approach_quality + 0.1×task_fitness reflect the reality that an 85% quality grasp with easy access often succeeds more reliably than a 95% quality grasp requiring precise positioning near obstacles.

For unknown objects, local geometry analysis provides fast heuristics. Detecting parallel surfaces via point cloud analysis generates antipodal grasp candidates even without global object models. Principal component analysis identifies object orientation, suggesting natural grasp axes. Curvature-based approaches favor regions with appropriate surface geometry for the gripper. These geometric methods typically run in 10-100ms and provide reasonable grasps for convex objects, though they struggle with complex geometries where learning-based approaches excel.

## UFactory Lite 6 brings comprehensive depth camera support

The Lite 6 represents an excellent match for your application, offering industrial capabilities at educational pricing with exceptional software support. Its 6-DOF kinematics, 440mm reach, and 600g payload accommodate typical assistive robotics scenarios, while ±0.5mm repeatability provides sufficient accuracy for most tabletop tasks.

**The official ufactory_vision repository eliminates integration guesswork.** This GitHub repository contains complete, working implementations of vision-guided grasping specifically for the Lite 6 with RealSense D435/D435i cameras. The critical files run_rs_grasp_lite6.py and run_depthai_grasp_lite6.py provide end-to-end pipelines from camera capture through grasp execution. This means you can clone the repository, install dependencies, and run actual pick and place on your hardware within hours—a stark contrast to platforms requiring custom integration of each component.

The Python SDK provides clean, intuitive control. After creating an XArmAPI instance with the robot's IP address, motion commands use either joint angles or Cartesian coordinates. The key insight: Mode 0 (position control) suffices for most pick and place applications, providing point-to-point motion with configurable speed and acceleration. More advanced modes enable velocity control, online replanning, and servo streaming for specialized requirements.

**Two architectural approaches suit different project needs.** The MoveIt-based approach (d435i_findobj2d_xarm_moveit_planner.launch) provides collision avoidance, singularity handling, and robust path planning through the ROS ecosystem. This becomes essential for complex environments with obstacles or workspace constraints. The direct API approach (d435i_findobj2d_xarm_api.launch) offers lower latency and simpler deployment, working well for structured environments with predictable workspaces. For your initial assistive robotics deployment, starting with the API approach enables faster iteration, with migration to MoveIt as requirements grow.

Gripper control integrates seamlessly. The Lite 6 two-finger parallel gripper uses pulse units 0-850 (closed to open) with configurable speed 1-5000. Simple service calls or Python SDK functions command gripper motion, enabling complete pick-place sequences. The vacuum gripper alternative suits flat objects or situations where parallel-jaw grasping proves challenging. Payload configuration via set_tcp_load() and coordinate offset via set_tcp_offset() ensure accurate motion planning and collision detection.

**Hand-eye calibration determines system accuracy.** The repository includes launch files for automated calibration using easy_handeye with ArUco markers. The process involves moving the robot to 15-20 diverse poses while the fixed camera observes the end-effector-mounted marker (or vice versa for eye-to-hand). The calibration computes the transformation from end-effector to camera frame, which then enables converting detected object positions from camera coordinates to robot base coordinates. Proper calibration typically achieves better than 2mm accuracy across the workspace—sufficient for grasping objects 50mm+ in size.

Hardware installation requires attention to details. USB 3.1 direct connection to the control PC avoids bandwidth limitations from hubs. Static IP configuration (typically 192.168.1.xxx subnet) ensures reliable communication with sub-2ms ping latency. Camera mounting needs rigid attachment to prevent calibration drift—the official camera stand or custom 3D-printed brackets serve well. Most importantly, collision detection configuration (sensitivity 3-5) provides safety margins during development while avoiding false triggers during normal operation.

## ROS 2 versus non-ROS approaches present clear trade-offs

Your decision between ROS 2 and standalone Python significantly impacts development trajectory and system capabilities. Both approaches can deliver working pick and place systems, but the optimal choice depends on project scope, team composition, and long-term goals.

**ROS 2 with MoveIt 2 provides industrial-grade manipulation infrastructure.** The ecosystem includes sophisticated motion planning via OMPL, collision checking, trajectory optimization, and standardized interfaces for perception and control. The MoveIt Task Constructor framework specifically enables complex pick and place sequences through a stage-based architecture—current state, approach object, grasp, lift, move to place, release, retreat. Each stage handles failure recovery and interfaces cleanly with the next. For production systems or research requiring robust autonomous operation, this infrastructure proves invaluable.

The learning curve presents the primary barrier. Reaching basic competency requires 2-4 weeks of dedicated learning: understanding nodes, topics, services, actions, the colcon build system, coordinate transforms via tf2, and basic MoveIt usage. Intermediate competency—creating custom packages, configuring motion planners, integrating perception pipelines—demands 1-3 months. Advanced usage with Task Constructor, custom planners, and production deployment typically requires 3-6 months of accumulated experience. The Construct and Udemy offer structured courses ($50-100) that significantly accelerate this timeline.

**Non-ROS Python development starts faster but requires building more infrastructure.** You can capture RealSense data, process point clouds with Open3D, and command the Lite 6 via its Python SDK within hours of starting—no framework installation, no build systems, no distributed architecture to understand. The UFACTORY vision repository exemplifies this approach: standalone Python scripts that directly call the xArm SDK. For proof-of-concept development or single-purpose applications, this simplicity enables rapid iteration.

The crossover point arrives when complexity accumulates. A simple pick and place cycle—detect object, plan grasp, execute motion—requires comparable effort in both approaches. However, adding capabilities like obstacle avoidance, multi-sensor fusion, complex motion sequences, or parallel robot operation strongly favors ROS 2. The non-ROS approach requires custom implementations of motion planning, collision detection, transform management, and system orchestration—components that ROS 2 provides as mature, tested packages.

**Performance considerations rarely determine the choice.** ROS 2 achieves 100-500 microsecond inter-process latency and supports control loops at 100-1000 Hz—far exceeding manipulation requirements where perception and planning consume 100-2000ms per cycle. Non-ROS approaches achieve slightly lower latency (10-50 microseconds for direct socket communication) but manipulation tasks rarely benefit from sub-millisecond timing. The algorithm choices—grasp detection methods, planning approaches, filtering parameters—impact performance orders of magnitude more than middleware overhead.

For your assistive robotics application starting without ROS experience, the pragmatic path begins with the non-ROS UFACTORY examples to achieve quick wins and understand the problem space. This provides working pick and place in days rather than weeks. As requirements evolve—adding navigation, multi-robot coordination, or complex task sequences—the transition to ROS 2 becomes worthwhile. The good news: the Lite 6 supports both approaches with equal documentation quality, avoiding vendor lock-in to either path.

The ROS 2 advantages manifest in team scaling and long-term maintainability. Multiple developers can work on perception, planning, and control modules independently through standardized interfaces. New team members leverage existing knowledge of ROS conventions rather than learning custom architectures. Production deployment benefits from mature tools like rosbag for data logging, RViz for visualization, and ros2_control for hardware abstraction. These factors matter more for multi-year projects than quick prototypes.

## Calibration accuracy determines system performance

Mechanical precision matters little if the coordinate transformations introducing 10mm errors. Proper calibration procedures and validation ensure the complete sensor-robot chain maintains accuracy.

**Hand-eye calibration establishes the critical camera-to-robot transformation.** For eye-in-hand mounting, this computes the fixed transformation from robot end-effector to camera frame. The process collects 15-20 robot poses where the camera observes a calibration target (ArUco marker or checkerboard), computing the transformation that best satisfies the constraint equation AX = XB. The Daniilidis algorithm using dual quaternions provides the most robust results, handling noisy data better than earlier methods like Tsai-Lenz.

The easy_handeye ROS package simplifies this substantially. Its GUI enables moving the robot to diverse poses, capturing samples with a single click, and automatically computing the calibration. The key to accuracy: pose diversity. Collect samples spanning a half-sphere of orientations with rotations exceeding 60° and varying across multiple axes. Settling time of 2+ seconds per pose prevents motion blur. The marker should fill 25%+ of the image for reliable detection. Following these guidelines achieves rotation error under 0.5° and translation error under 2mm—adequate for grasping objects 20mm+ in size.

**Common calibration errors have specific symptoms.** If the robot consistently misses by similar amounts across the workspace, suspect translation error in the hand-eye calibration—often caused by poor camera intrinsics or insufficient pose diversity. If errors vary by position, the robot's own kinematic calibration may be inaccurate. Orientation errors manifest as correct position but wrong approach angles. Systematic debugging involves testing with known object positions and comparing predicted versus actual locations.

Camera intrinsic calibration usually relies on factory values for the RealSense D435. Intel pre-calibrates each unit, storing parameters in firmware. These suffice for most applications—measured reprojection errors typically fall below 0.5 pixels. The dynamic calibration tool can optimize performance if accuracy degrades, though this rarely proves necessary. Traditional checkerboard calibration using OpenCV remains an option for maximum precision, requiring 10-20 images from varied angles.

**Depth-to-RGB alignment requires accurate camera extrinsics.** The RealSense SDK computes this using factory calibration of the translation and rotation between IR and RGB sensors. Enabling rs.align(rs.stream.color) applies this transformation, ensuring depth and color pixels correspond. Without alignment, RGB-based object detection and depth-based localization reference different coordinate frames, introducing systematic errors. Always use aligned depth frames for manipulation applications.

The complete coordinate chain flows: camera frame → end-effector frame (hand-eye calibration) → base frame (forward kinematics) → world frame (base placement). Each transformation accumulates error. Testing with known reference positions validates the entire chain. Place objects at measured locations, detect them with the vision system, command the robot to the detected position, and measure actual placement. Errors exceeding 5mm warrant recalibration.

**Environmental factors affect calibration stability.** Temperature changes alter mechanical dimensions and electronic characteristics. The RealSense D435 requires 10-15 minute warmup for thermal stability. Vibration or mechanical shock can shift camera mounting, invalidating calibration. Production systems implement periodic validation: attempt to grasp a known object at a fixed location, recording success rates and position errors. Degradation signals recalibration needs before it causes repeated failures.

Lighting dramatically impacts depth camera performance despite IR-based sensing. Direct sunlight contains substantial IR energy, overwhelming the projected pattern. Very dark objects reflect IR poorly, reducing signal-to-noise ratio. Highly reflective surfaces create spurious returns. For assistive robotics in home environments, controlling lighting proves challenging. Strategies include: ensuring operation away from windows during direct sun, using diffuse LED lighting rather than spotlights, and enabling the spatial and temporal filters to reduce noise. The AI-based 3D vision systems emerging in 2025 show promise for lighting-independent operation by reasoning about 3D geometry rather than raw intensity values.

## Practical deployment requires systematic problem solving

Moving from working demonstrations to reliable operation demands addressing failure modes, optimizing performance, and implementing robust error handling.

**Occlusion handling separates research systems from production-ready solutions.** Self-occlusion occurs naturally when objects touch or overlap. The segmentation pipeline must separate individual items from point cloud clusters—Euclidean clustering with appropriate tolerance (0.02m) handles this reasonably well. Partial object views provide sufficient information for grasping if the visible surface includes graspable regions. Multi-view approaches capture the scene from 2-3 camera positions, fusing point clouds to reduce occlusion. For bin picking applications where objects pile in cluttered containers, iterative approaches work well: grasp and remove top-layer objects first, re-scan, then access previously occluded items.

Gripper self-occlusion—where the gripper itself blocks the camera's view of the grasp point—affects eye-in-hand systems. Offsetting the camera mount 50-100mm from the gripper center and tilting downward 15-30° usually provides sufficient clearance. Alternatively, plan grasps based on pre-grasp scans from offset positions, then move to the grasp pose without requiring visibility at contact. Proprioceptive feedback from force-torque sensors or gripper encoders can guide final positioning when vision becomes occluded, though the Lite 6 lacks built-in force sensing.

**Speed versus accuracy trade-offs pervade the system.** Depth camera capture time ranges from 33ms at 30 FPS to 100ms for higher quality or HDR modes. Point cloud processing takes 50-200ms depending on resolution and filter settings. Grasp detection via GR-ConvNet requires 20-50ms, while motion planning consumes 0.5-5 seconds. Robot motion itself dominates cycle time at 2-10 seconds per pick-place action. Optimizing the wrong component wastes effort—reducing camera latency from 100ms to 50ms provides minimal benefit when motion planning takes 2 seconds.

The critical optimization targets perception reliability and planning success rate. Processing at 10 Hz instead of 30 Hz usually suffices since the robot moves orders of magnitude slower than the vision pipeline. Velocity scaling during vision-guided approach (30-50% of maximum) improves accuracy more than increasing sensing frequency. Multiple grasp candidates with graceful fallback handle detection uncertainties better than optimizing single-grasp execution time.

Surface properties create challenging cases. Transparent objects remain fundamentally difficult for active stereo systems—the IR pattern passes through rather than reflecting. Backlighting that creates silhouettes, UV fluorescence marking, or light powder coating (acceptable for some assistive robotics scenarios) provides workarounds. Highly reflective objects like polished metal or glossy plastics create spurious depth readings. Polarized lighting with crossed polarization filter reduces specular reflections. Diffuse LED panel lighting rather than directional spotlights minimizes hotspots.

**Error handling determines whether failures cascade or recover gracefully.** Vision failures (no objects detected) should trigger re-scanning from alternate viewpoints or explicit failure reporting rather than attempting blind grasps. IK failures (no valid robot configuration) call for trying the next-best grasp candidate from a ranked list. Motion planning failures suggest simplifying by moving to intermediate waypoints or relaxing constraints. Grasp failures detected via gripper sensors warrant retry attempts or human notification in assistive robotics contexts.

Cycle time analysis reveals optimization opportunities. Instrument each pipeline stage: capture 50ms, preprocessing 100ms, segmentation 150ms, grasp detection 50ms, ranking and IK 200ms, motion planning 2000ms, execution 5000ms. This profile immediately shows motion dominates total time. Parallelizing planning (computing next grasp while current motion executes) cuts cycles by 2-3 seconds. Optimizing perception from 350ms to 150ms saves less than 1% of total time unless multiple perception iterations occur.

Safety systems protect humans and equipment. The Lite 6's collision detection monitors joint torques, stopping motion when unexpected forces arise. Setting sensitivity 3-5 provides safety margins without triggering on normal contact forces. Workspace limits enforced in software prevent wild motions from programming errors. Watchdog timers detect hung vision processing, preventing indefinite waits. For assistive robotics involving human interaction, conservative velocity limits (30-50% maximum) and multiple confirmation steps before executing near people prove essential.

## Implementation roadmap accelerates development

Starting with a structured approach minimizes time exploring dead ends and ensures critical steps receive attention.

**Phase one establishes the foundation in 1-2 days.** Install Python SDK and verify communication with the Lite 6 via basic jog commands. Install RealSense SDK and capture test images to confirm camera function. Mount the camera securely in an approximate position—precise calibration follows later. Test basic gripper control to ensure the full mechanical chain operates. This phase validates hardware before investing in software complexity.

Phase two implements basic vision in 2-3 days. Capture aligned RGB-D frames from the RealSense. Generate point clouds using camera intrinsics and visualize with Open3D to verify geometry. Implement basic segmentation: passthrough filter to workspace bounds, plane removal via RANSAC, Euclidean clustering for object separation. At this stage, detecting object centroids suffices—no grasp planning yet. Manually verify detected positions match physical locations within several centimeters.

**Phase three integrates grasp detection in 2-4 days.** Clone the ufactory_vision repository and run the provided GGCNN examples on your hardware. This proves the complete pipeline works. Then study the code to understand the integration points: how point clouds become grasp poses, how those transform to robot coordinates, how the Lite 6 executes the resulting motions. Modify parameters like cluster tolerance, grasp quality threshold, and motion speeds to match your objects and environment. Document what works and what fails—this informs future refinements.

Phase four performs hand-eye calibration in 1 day. Print a large ArUco marker (100mm square) and mount it rigidly on the workspace. Use easy_handeye to collect 15-20 robot poses with varied orientations. The automatic calibration computes the camera-to-gripper transformation. Validate by placing objects at known positions, detecting them, commanding the robot to detected coordinates, and measuring actual reaching accuracy. Errors under 5mm indicate successful calibration. If errors exceed 10mm, recalibrate with more diverse poses or check for mechanical issues.

**Phase five implements complete pick and place in 3-5 days.** Define approach poses (50mm above grasp points) to avoid collisions during descent. Implement the pick sequence: move to approach, descend to grasp, close gripper, lift, move to place approach, descend, open gripper, retreat. Add error checking at each stage: verify motion completion before advancing, confirm gripper state changes, validate object detection before attempting grasp. Test with progressively challenging scenarios: single known object in open space, multiple objects requiring selection, objects near workspace edges, objects of varying sizes.

Phase six addresses robustness and deployment in 2+ weeks. Implement graceful failure handling: retry with alternative grasps, multi-view sensing for occluded objects, human notification when automation fails. Optimize performance: measure cycle times, parallelize computation where possible, tune parameters for your specific objects. Deploy in the target environment and collect failure data—this reveals issues absent in controlled testing. Iterate based on real-world performance.

For assistive robotics applications, human interaction receives special attention. Design the system to fail safely: sudden stops must not endanger users, dropped objects should fall safely, unexpected behavior should trigger immediate halts. Provide clear feedback about system state: LEDs indicating processing, sounds signaling completion or errors, displays showing what the robot sees. Human override capability proves essential—users must be able to pause or cancel operations at any time.

**Tool selection matching your constraints optimizes productivity.** Starting without ROS 2 experience, begin with the standalone Python implementation in ufactory_vision. This provides working pick and place in days, building understanding of the problem space. As requirements expand—adding obstacle avoidance, task planning, or multi-robot coordination—consider migrating to ROS 2. The Lite 6 supports both paths equally, and Open3D point cloud processing translates directly between approaches.

For grasp detection, GR-ConvNet pretrained on Cornell or Jacquard datasets provides strong baselines. The GGCNN implementation in ufactory_vision works immediately. If your objects differ substantially from typical datasets (unusual shapes, materials, or sizes), fine-tuning on 100-500 examples of your specific items boosts success rates significantly. The PyTorch implementation enables transfer learning with modest GPU resources.

Motion planning initially uses the Lite 6's built-in mode 0 position control—simple point-to-point motion suffices for uncluttered environments. As complexity grows, MoveIt 2 integration provides collision avoidance and sophisticated planning. The xarm_ros package includes MoveIt configuration for the Lite 6, enabling this transition without starting from scratch.

## Achieving robust assistive robotics systems

Your specific application context—assistive robotics for users in less controlled environments—emphasizes reliability over speed. Success requires systems that handle variability, fail gracefully, and maintain safety.

**Object diversity demands generalization.** Assistive tasks involve varied objects: containers, utensils, personal items, packaging. Training on diverse datasets like GraspNet-1Billion provides better generalization than Cornell alone. Augmenting training data with domain randomization—varying lighting, adding synthetic clutter, randomizing object poses—improves robustness. For critical objects, collecting 50-100 examples and fine-tuning detection models ensures reliable recognition.

Environmental variability exceeds laboratory settings. Lighting changes throughout the day, backgrounds include visual clutter, surfaces vary in texture and reflectivity. Robust systems tolerate these variations through multi-modal sensing (RGB + depth + force), conservative decision thresholds (only attempt grasps with high confidence), and fallback strategies (request human help rather than failing silently). The 10-15 Hz perception rate enables capturing multiple frames before committing to actions, improving reliability through temporal consistency checks.

**Human-in-the-loop design proves essential for assistive applications.** Full automation remains elusive—unexpected situations inevitably arise. Designing for human intervention rather than treating it as failure enables graceful degradation. Clear status indication shows users what the system perceives and plans. Simple override controls pause operations safely. Explicit confirmation steps before critical actions prevent automation surprises. This collaborative approach combines robot precision and consistency with human judgment and adaptability.

Safety mechanisms protect users and equipment. Conservative velocity limits (30% maximum) provide reaction time for human intervention. Collision detection configured at moderate sensitivity (3-4) stops motion upon unexpected contact without false triggers during normal operation. Workspace boundaries prevent reaching into unsafe zones. Software watchdogs detect hung processes, preventing indefinite waits. These layers provide defense in depth against failure modes.

Maintenance and monitoring sustain long-term operation. Logging pick success rates, cycle times, and failure modes reveals degradation before it becomes critical. Periodic validation with known reference objects checks calibration accuracy. Camera lens cleaning maintains image quality—dust accumulation gradually reduces performance. Firmware updates for both camera and robot address discovered issues. Budget for 1-2 hours monthly maintenance time.

**Performance expectations should reflect realistic constraints.** Laboratory demonstrations achieve 95%+ success on known objects in controlled settings. Real-world assistive robotics more realistically targets 80-90% success across diverse objects in variable environments. This proves acceptable when failures invoke human assistance rather than causing damage. Cycle times of 10-30 seconds per pick-place satisfy assistive applications where reliability and safety matter far more than speed. The system supplements human capability rather than replacing it entirely.

Your RealSense D435 and UFactory Lite 6 combination provides capable hardware for assistive robotics. The camera's 2-5mm accuracy at typical working distances exceeds requirements for grasping objects 20mm+ in size. The Lite 6's 600g payload accommodates common household items, while its 440mm reach covers tabletop workspaces. The ±0.5mm repeatability proves adequate when combined with vision-guided correction. Most importantly, the extensive software support and working examples compress development time from months to weeks.

The path forward involves starting simple, validating incrementally, and expanding systematically. Run the ufactory_vision examples first to prove the basic pipeline works on your hardware. Then adapt to your specific objects and environment. Add capabilities—multi-view sensing, improved grasp detection, obstacle avoidance—as needs become clear from real-world testing. Engage with the active communities around RealSense and UFACTORY to leverage collective knowledge. Your assistive robotics application will iterate toward robustness through accumulated refinements rather than attempting perfect initial design.

Success emerges from balancing ambitious goals with practical constraints, sophisticated algorithms with reliable execution, and autonomous operation with human collaboration. The technology stack detailed in this report provides the building blocks. Your application-specific knowledge—understanding user needs, environmental constraints, and acceptable trade-offs—guides their assembly into systems that genuinely assist rather than merely demonstrate capability.